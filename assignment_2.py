# -*- coding: utf-8 -*-
"""Assignment_2.ipynb

Automatically generated by Colaboratory.


# ***What is EDA?***

EDA stands for 'Exploratory Data Analysis', and is an approach that mostly employes graphical techniques to

1) Maximize insight

2) Uncover underlying structure

3) Detect outliers and anomalies

4) Test assumptions

# ***There are two EDA approaches that can be undertaken***

1) Graphical: Histogram, Bee Swarm Plots, Emprical Cumulative Distribution Function (ECDF).

2) Statistical: EDA Summary Statistics: Mean, Median, and Outliers.
"""

# Import datasets
from sklearn import datasets
# Import pandas to convert dataset to dataframe
import pandas as pd
# Import numpy
import numpy as np
# Import plotting modules
import matplotlib.pyplot as plt
import seaborn as sns

iris = datasets.load_iris()
X = iris.data
y = iris.target
df = pd.DataFrame(X, columns=iris.feature_names) 
full_df = df.copy()
full_df['Target']=y
full_df['Target_Text'] = full_df.Target.apply(lambda x: {0: 'Setosa', 1: 'Versicolour',
                                                         2: 'Virginica'}[x])

# Splitting the Dataframe into series based on the flower spieces
setosa_petal_length = full_df[full_df['Target'] == 0]['petal length (cm)']  # Iris-Setosa
setosa_petal_width = full_df[full_df['Target'] == 0]['petal width (cm)']  # Iris-Setosa

versicolor_petal_length = full_df[full_df['Target'] == 1]['petal length (cm)']  # Iris-Versicolour
versicolor_petal_width = full_df[full_df['Target'] == 1]['petal width (cm)']  # Iris-Versicolour

virginica_petal_length = full_df[full_df['Target'] == 2]['petal length (cm)']  # Iris-Virginica
virginica_petal_width = full_df[full_df['Target'] == 2]['petal width (cm)']  # Iris-Virginica

"""# **Histogram**
Creating a histogram provides a visual representation of data distribution.

**Benefits:**
- Can display large amount of data and frequency of data values.
- Median and distribution can be determined by a histogram.
- It clearly shows outliers or gaps in the data.
"""

# Plotting IRIS Data

# Set default Seaborn style
sns.set()

# Compute number of data points: n_data
n_data = len(versicolor_petal_length)

# Number of bins is the square root of number of data points: n_bins
n_bins = np.sqrt(n_data)

# Convert number of bins to integer: n_bins
n_bins = int(n_bins)

plt.hist(setosa_petal_length, bins=n_bins)
# Label axes
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('count')

# Show histogram
plt.show()
plt.hist(versicolor_petal_length, bins=n_bins)
# Label axes
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('count')

# Show histogram
plt.show()
plt.hist(virginica_petal_length, bins=n_bins)

# Label axes
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('count')

# Show histogram
plt.show()

"""# **Bee Swarm Plots**
It emphasizes individual points in a distribution instead of binning them like a histogram. It is much more visually distinctive than a histogram, especially when comparing.

**Benefits:**
- Very easy to spot all outliers.
- Can differentiate between spieces effortlessly.
- Only one graph is used to compare all speices.
"""

# Create bee swarm plot with Seaborn's default settings
_ = sns.swarmplot(x='Target_Text', y='petal length (cm)', data=full_df) 

# Label the axes
_ = plt.xlabel('species')
_ = plt.ylabel('petal length (cm)')

# Show the plot
plt.show()

"""# **Emprical Cumulative Distribution Function (ECDF)**
ECDF allows you to plot a feature of your data in order from least to greatest and see the whole feature as if is distributed across the data set.

**Benefits:**
- Shows variables and their corresponding probability.
- Similar series will be displayed close or overlapping each other.
"""

# ECDF
def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    # Number of data points: n
    n = len(data)

    # x-data for the ECDF: x
    x = np.sort(data) 

    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n

    return x, y

# Compute ECDFs
x_set, y_set = ecdf(setosa_petal_length)
x_vers, y_vers = ecdf(versicolor_petal_length)
x_virg, y_virg = ecdf(virginica_petal_length)



# Plot all ECDFs on the same plot
plt.plot(x_set, y_set, marker='.', linestyle='none')
plt.plot(x_vers, y_vers, marker='.', linestyle='none')
plt.plot(x_virg, y_virg, marker='.', linestyle='none')

# Annotate the plot
plt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Display the plot
plt.show()

"""# **Boxplot**
Boxplot displays a summary of a large amount of data in five numbers. These numbers include the median, upper quartile, lower quartile, minimum and maximum data values.

**Benefits:**
- Shows outliers clearly.
- Allows for quicker comaprison.
"""

# Create box plot with Seaborn's default settings
_ = sns.boxplot(x='Target_Text', y='petal length (cm)', data=full_df)

# Label the axes
_ = plt.xlabel('species')
_ = plt.ylabel('petal length (cm)')

# Show the plot
plt.show()

"""# **Wine Dataset**"""

wine = datasets.load_wine()
X = wine.data
y = wine.target
print(wine['DESCR'])

z = pd.DataFrame(wine.data)

print(z.info())

df = pd.DataFrame(X, columns=wine.feature_names) 
full_df = df.copy()

full_df['Target']=y
full_df['Target_Text'] = full_df.Target.apply(lambda x: {0: 'Class A', 1: 'Class B',
                                                         2: 'Class C'}[x])
full_df.tail(n=3)

calss_a_alcohol = full_df[full_df['Target'] == 0]['alcohol']  # Wine-Class-A
class_a_flavanoids = full_df[full_df['Target'] == 0]['flavanoids']  # Wine-Class-A

calss_b_alcohol = full_df[full_df['Target'] == 1]['alcohol']  # Wine-Class-B
class_b_flavanoids = full_df[full_df['Target'] == 1]['flavanoids']  # Wine-Class-B

calss_c_alcohol = full_df[full_df['Target'] == 2]['alcohol']  # Wine-Class-C
class_c_flavanoids = full_df[full_df['Target'] == 2]['flavanoids']  # Wine-Class-C

# Compute ECDFs
x_a, y_a = ecdf(calss_a_alcohol)
x_b, y_b = ecdf(calss_b_alcohol)
x_c, y_c = ecdf(calss_c_alcohol)



# Plot all ECDFs on the same plot
plt.plot(x_a, y_a, marker='.', linestyle='none')
plt.plot(x_b, y_b, marker='.', linestyle='none')
plt.plot(x_c, y_c, marker='.', linestyle='none')

# Annotate the plot
plt.legend(('Class A', 'Class B', 'Class C'), loc='lower right')
_ = plt.xlabel('alcohol')
_ = plt.ylabel('ECDF')

# Display the plot
plt.show()

#Create bee swarm plot with Seaborn's default settings
 
_ = sns.swarmplot(x='Target_Text', y='flavanoids', data=full_df) 

# Label the axes
_ = plt.xlabel('Classes')
_ = plt.ylabel('flavanoids')

# Show the plot
plt.show()

# Create box plot with Seaborn's default settings
_ = sns.boxplot(x='Target_Text', y='color_intensity', data=full_df)

# Label the axes
_ = plt.xlabel('Classes')
_ = plt.ylabel('Color Intensity')

# Show the plot
plt.show()

sns.pairplot(full_df,hue='Target_Text', palette="husl",
             vars=['color_intensity', 'alcohol', 'flavanoids',
                   'malic_acid']);

# Specify array of percentiles: percentiles
percentiles = np.array([2.5, 25, 50, 75, 97.5])

# Compute percentiles
alc_p = np.percentile(calss_a_alcohol, percentiles)

# Print the result
print(alc_p)


calss_a_alcohol.sort_values(ascending=True)[-4:]

# Plot the ECDF
_ = plt.plot(x_a, y_a, '.')
_ = plt.xlabel('alcohol')
_ = plt.ylabel('ECDF')

# Overlay percentiles as red diamonds.
_ = plt.plot(alc_p, percentiles/100, marker='D', color='red',
         linestyle='none')

# Show the plot
plt.show()

cbf = class_b_flavanoids.cumsum()

cbf.plot()

class_b_flavanoids.plot()

wine = load_wine()

X = pd.DataFrame(wine.data)
y = pd.Series(wine.target)

print(X.info())

# Is a data set that requires scaling
# If you use a tree-based algorithm
# Analyze without pretreatment
pd.options.display.max_columns=13
print(X.describe())

print(y.value_counts())
print(y.value_counts() / len(y))

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
        X.values, y.values, stratify=y.values, 
        random_state=42)

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(
        random_state=1).fit(X_train, y_train)

# Check the accuracy of the model
print(' : ', model.score(X_train, y_train))
print(' Evaluation: ', model.score(X_test, y_test))

# Check the model's precision / reproducibility
from sklearn.metrics import classification_report

pred_train = model.predict(X_train)
pred_test = model.predict(X_test)

print('classification_report - learning')
print(classification_report(y_train, pred_train))

print('classification_report - test')
print(classification_report(y_test, pred_test))

# Check attribute importance
print(' Importance: \n', model.feature_importances_)

import numpy as np
from matplotlib import pyplot as plt

plt.figure(figsize=(10,7))

def plot_feature_importances(model, feature_names):
    n_features = len(feature_names)
    plt.barh(range(n_features), 
             model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), feature_names)
    plt.xlabel("feature_importances")
    plt.ylabel("feature")
    plt.ylim(-1, n_features)

plot_feature_importances(model, wine.feature_names)

# check graph
from sklearn.tree import export_graphviz
export_graphviz(model, out_file='./load_wine.dot',                
                feature_names=wine.feature_names, 
                filled=True)

import graphviz
from IPython.display import display

with open('./load_wine.dot', encoding='utf-8') as f:
    dot_graph = f.read()
    
display(graphviz.Source(dot_graph))

load=load_wine()
x=load.data
y=load.target
print(load_wine.__doc__)
print('---------')
print(x.shape)
print('---------')
print(y.shape)
print('---------')
plt.figure(figsize=(12,10))
tick=np.linspace(1,179,178,dtype='int')
color=[]
for i in range(len(y)):
    if y[i]==0:
        color.append('r')
        continue
    if y[i]==1:
        color.append('g')
        continue
    if y[i]==2:
        color.append('b')
plt.subplot(221)
plt.scatter(tick,x[:,0],c=color)
plt.subplot(222)
plt.scatter(tick,x[:,1],c=color)
plt.subplot(223)
plt.scatter(tick,x[:,2],c=color)
plt.subplot(224)
plt.scatter(tick,x[:,3],c=color)

X, y = load_wine(return_X_y=True)
#print(X.shape) (178,13)
#print(y.shape) (178,)

names = load_wine().feature_names

plt.figure(figsize = (50,30))

for i in range(13):
    plt.subplot(4,4, i+1)
    plt.scatter(X[:, i], y)
    plt.xlabel(names[i])
    plt.ylabel('Classification')
plt.show()